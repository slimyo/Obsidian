**æ€»å…± 52 å‘¨ï¼ŒæŒ‰å‘¨æ‹†åˆ†ï¼Œæ¯å‘¨ç›®æ ‡æ˜ç¡®ã€äº§å‡ºæ˜ç¡®ã€å‚è€ƒèµ„æºå¯è®¿é—®ã€‚**

---

# ğŸŸ¦ **å­¦ä¹ æ€»ç›®æ ‡ï¼ˆä¸€å¹´åèƒ½åšåˆ°ï¼‰**

- å†™å‡º **CPU é«˜æ€§èƒ½å†…æ ¸ï¼ˆSIMD + cache blockingï¼‰**
    
- å†™å‡º **GPU Tiled GEMMï¼ˆCUTLASS é£æ ¼ï¼‰**
    
- ç†è§£ **PyTorch ATen / DispatchKey / TensorImpl**
    
- ç†è§£ **vLLM / Triton / TensorRT / XLA çš„æ ¸å¿ƒè·¯å¾„**
    
- ç†Ÿæ‚‰ **åˆ†å¸ƒå¼è®­ç»ƒåŸºæœ¬æœºåˆ¶ï¼ˆDDPã€ZeROã€pipelineã€tensor parallelï¼‰**
    
- èƒ½èƒœä»» **AI Infra / HPC / CUDA å†…æ ¸å·¥ç¨‹å¸ˆ** çš„å·¥ä½œ
    

---

# ğŸŸ¥ **é˜¶æ®µ 0ï¼šå‡†å¤‡é˜¶æ®µï¼ˆç¬¬ 1 å‘¨ï¼‰**

---

# **ğŸ“˜ Week 1ï¼šç¯å¢ƒæ­å»º + Linux å·¥å…·ï¼ˆ40hï¼‰**

ç›®æ ‡ï¼šæ­å¥½æœªæ¥ä¸€æ•´å¹´å¼€å‘ç¯å¢ƒ + ç†Ÿæ‚‰å·¥å…·é“¾ã€‚

**ä»»åŠ¡**

- å®‰è£… Linuxï¼ˆUbuntu 22.04 LTSï¼‰
    
- é…ç½® CUDA / NSight Systems / Nsight Compute
    
- é…ç½® gdb / perf / strace / flamegraph
    
- é…ç½® clang, gcc, ninja, CMake
    
- å­¦ä¼šå†™ä¸€ä¸ªæ ‡å‡†çš„ CMake é¡¹ç›®éª¨æ¶
    
- é…ç½® VSCode + clangd
    
- é…ç½® Python ç¯å¢ƒï¼ˆPyTorch nightly / CUDA Toolkitï¼‰
    

**äº§å‡º**

- ä¸€ä¸ªå®Œæ•´å¯ç”¨çš„ dev ç¯å¢ƒä»“åº“
    
- èƒ½ç‹¬ç«‹ç”¨ perf record & perf report æ£€æŸ¥ç®€å•ç¨‹åº
    

**èµ„æº**

- CUDA Toolkitï¼šdocs.nvidia.com/cuda
    
- Perf æ•™ç¨‹ï¼šbrendangregg.com/perf.html
    
- Nsight Systems docsï¼šdocs.nvidia.com/nsight-systems
    

---

# ğŸŸ§ **é˜¶æ®µ 1ï¼šç³»ç»Ÿä¸æ€§èƒ½åŸºç¡€ï¼ˆWeek 2â€“7ï¼Œå…± 6 å‘¨ï¼‰**

é‡ç‚¹ï¼šæ“ä½œç³»ç»ŸåŸºç¡€ã€Linux ç³»ç»Ÿè°ƒç”¨ã€threadingã€perfã€CPU å¾®æ¶æ„ã€‚

---

# **ğŸ“˜ Week 2â€“4ï¼šLinux/UNIX ç³»ç»Ÿç¼–ç¨‹ï¼ˆ120hï¼‰**

ä¹¦ç±ï¼š

- **APUEï¼ˆAdvanced Programming in the UNIX Environmentï¼‰**
    
- **TLPIï¼ˆThe Linux Programming Interfaceï¼‰**
    

å­¦ä¹ å†…å®¹ï¼ˆæŒ‰å‘¨åšï¼‰


|ç« èŠ‚å· (Chapter)|å†…å®¹ / æ¨¡å—|æ¨èç†ç”±|
|---|---|---|
|**4â€“5**|æ–‡ä»¶ I/O åŸºç¡€ & æ·±å…¥ (open/read/write/close/fcntl / pread/pwrite / scatter-gather / nonblocking I/O / large file)|åŸºæœ¬ I/Oï¼Œæ˜¯æ‰€æœ‰ç³»ç»Ÿ + æ•°æ®åŠ è½½/å­˜å‚¨ + æ—¥å¿— + æ–‡ä»¶äº¤äº’åŸºç¡€|
|**6**|è¿›ç¨‹ (process basics)|ç†è§£è¿›ç¨‹æ¨¡å‹ã€è™šæ‹Ÿåœ°å€ç©ºé—´ã€å‘½ä»¤è¡Œï¼ç¯å¢ƒï¼å †æ ˆå¸ƒå±€ï¼Œå¯¹ä»»ä½•ç³»ç»Ÿçº§/æœåŠ¡çº§éƒ½åŸºç¡€|
|**24â€“28**|è¿›ç¨‹åˆ›å»º / exec / fork / clone / process termination / wait / å­è¿›ç¨‹ç®¡ç†|dataloaderã€worker poolã€å¤šè¿›ç¨‹æœåŠ¡ / åˆ†å¸ƒå¼å¯åŠ¨ç­‰ï¼Œéƒ½å¼ºä¾èµ– fork/exec/execve/clone|
|**29â€“33**|çº¿ç¨‹ (pthread)ã€åŒæ­¥ã€thread local storage|å¤šçº¿ç¨‹ I/Oã€å¼‚æ­¥ä»»åŠ¡ã€å¹¶å‘ kernelã€çº¿ç¨‹æ± ç­‰å¿…é¡»|
|**49â€“50**|Memory mapping / Virtual memory operations (mmap, mprotect, mlock, madvise, msync, shared memory mapping)|å¯¹å¤§æ¨¡å‹ / GPU pinned memory / æ•°æ®é¢„å¤„ç† / zero-copy IO éå¸¸å…³é”®|
|**56â€“60**|Socket API + TCP/UDP ç½‘ç»œåŸºç¡€ + Server è®¾è®¡|ç”¨äº RPC / inference server / distributed training / parameter server ç­‰ç½‘ç»œé€šä¿¡|
|**60 (I/O å¤šè·¯å¤ç”¨ / æœåŠ¡å™¨è®¾è®¡)**|epoll / select / poll / server model|æ„å»ºé«˜æ€§èƒ½ç½‘ç»œæœåŠ¡ (å¦‚ LLM inference æœåŠ¡) çš„åŸºç¡€|
|**20â€“22 / 23**|ä¿¡å· (signal) + è®¡æ—¶å™¨ / sleep /å®šæ—¶å™¨|å¯¹åå°ä»»åŠ¡ã€å¼‚æ­¥ I/Oã€è¶…æ—¶ã€èµ„æºç®¡ç†ã€å®šæ—¶ä»»åŠ¡ç®¡ç†é‡è¦|
|**43â€“48 / IPC / Pipe / FIFO / System V / POSIX IPC / Shared Memory / Semaphore / Message Queue / File Locking**|è¿›ç¨‹é—´é€šä¿¡ + å…±äº«å†…å­˜ + é”æœºåˆ¶|å¯¹å¤šè¿›ç¨‹ / å¤š worker / IPC / æ•°æ®å…±äº« /é”æœºåˆ¶æœ‰ç”¨ (å°¤å…¶åœ¨ HPCã€è®­ç»ƒ/æ¨ç†ç³»ç»Ÿ)|

### Week 2ï¼ˆ40hï¼‰

- è¿›ç¨‹ï¼šfork/exec/wait
    
- æ–‡ä»¶æè¿°ç¬¦ã€open/close/read/write
    
- ç›®å½•ã€æƒé™ã€stat
    

### Week 3ï¼ˆ40hï¼‰

- çº¿ç¨‹ï¼špthreadã€é”ã€æ¡ä»¶å˜é‡  
    -çº¿ç¨‹å®‰å…¨ã€æ­»é”ã€å†…å­˜å¯è§æ€§
    
- è¿›ç¨‹é—´é€šä¿¡ï¼špipe/socketpair/shared memory
    

### Week 4ï¼ˆ40hï¼‰

- mmap/mprotect/æ–‡ä»¶æ˜ å°„
    
- è™šæ‹Ÿå†…å­˜æœºåˆ¶
    
- signals (SIGALRMã€SIGCHLD)
    
- å®šæ—¶å™¨ timerfd
    

**äº§å‡º**

- è‡ªå·±å†™çš„ mini-shellï¼ˆæ”¯æŒ pipelineï¼‰
    
- ä½¿ç”¨ mmap å†™åªè¯»æ•°æ®åº“åŠ è½½å™¨
    
- pthread çº¿ç¨‹æ± 
    

---

# **ğŸ“˜ Week 5â€“7ï¼šCPU æ¶æ„ + æ€§èƒ½åˆ†æï¼ˆ120hï¼‰**

ä¹¦ç±ï¼š

- **CSAPP ç¬¬ 6 ç« ï¼ˆCacheï¼‰**
    
- **ã€Šè®¡ç®—æœºä½“ç³»ç»“æ„ï¼šé‡åŒ–ç ”ç©¶ã€‹Hennessy & Patterson**
    

å­¦ä¹ å†…å®¹

### Week 5

- Cache lineã€TLBã€page walk
    
- prefetchã€false sharing
    
- NUMA æ¶æ„
    

### Week 6

- perfã€VTuneã€FlameGraph
    
- CPU pipeline / branch predictor
    
- microbenchmark æ–¹æ³•
    

### Week 7

**é¡¹ç›®ï¼šå†™ CPU microbenchmark å¥—ä»¶**

- pointer chasing latency
    
- sequential vs random access
    
- L1/L2/L3 miss ratio
    
- branch mispredict
    

**äº§å‡º**

- custom profiler
    
- cache-friendly vs unfriendly demo
    
- å®Œæ•´ profiler æŠ¥å‘Šï¼ˆä½ çš„ç¬¬ä¸€ä¸ªä½œå“é›†é¡¹ç›®ï¼‰
    

---

# ğŸŸ¨ **é˜¶æ®µ 2ï¼šç°ä»£ C++ + æ¨¡æ¿å…ƒç¼–ç¨‹ï¼ˆWeek 8â€“12ï¼Œå…± 5 å‘¨ï¼‰**

ç›®æ ‡ï¼šè¾¾åˆ°èƒ½å¤Ÿé˜…è¯» Eigen/CUTLASS çš„ç¨‹åº¦ã€‚

---

# **ğŸ“˜ Week 8â€“9ï¼šC++ æ¨¡æ¿ã€CRTPã€ç¼–è¯‘æœŸï¼ˆ80hï¼‰**

å†…å®¹ï¼š

- templates / partial specialization
    
- SFINAE / Concepts
    
- type traits
    
- constexpr programming
    
- CRTP deep diveï¼ˆEigen çš„æ ¹åŸºï¼‰
    

---

# **ğŸ“˜ Week 10ï¼šè¡¨è¾¾å¼æ¨¡æ¿ï¼ˆ40hï¼‰**

å†…å®¹ï¼š

- operator é‡è½½æ„é€  AST
    
- æƒ°æ€§æ±‚å€¼
    
- inline å±•å¼€
    
- ä¸´æ—¶å¯¹è±¡ä¼˜åŒ–
    

é¡¹ç›®ï¼š  
**mini-eigenï¼ˆè¡¨è¾¾å¼æ¨¡æ¿ï¼‰**

---

# **ğŸ“˜ Week 11â€“12ï¼šSIMD + å†…å­˜å¸ƒå±€ï¼ˆ80hï¼‰**

å†…å®¹ï¼š

- AoS vs SoA
    
- std::simd
    
- AVX2/AVX512 intrinsic
    
- aligned load/store
    
- fma/unroll
    

é¡¹ç›®ï¼š

- SIMD ä¼˜åŒ– FMADD kernel
    
- SIMD ç‰ˆ GEMV
    
- SIMD å¯¹æ¯” naive æ€§èƒ½æŠ¥å‘Š
    

---

# ğŸŸ© **é˜¶æ®µ 3ï¼šæ•°å€¼è®¡ç®— + HPCï¼ˆWeek 13â€“20ï¼Œå…± 8 å‘¨ï¼‰**

ç›®æ ‡ï¼šå†™å‡ºä¸€ä¸ªå¯ç”¨çš„ GEMM å†…æ ¸ï¼ˆCPU ç‰ˆï¼‰ã€‚

---

# **ğŸ“˜ Week 13â€“15ï¼šæ•°å€¼åˆ†æåŸºç¡€ï¼ˆ120hï¼‰**

ææ–™ï¼š

- Trefethenã€ŠNumerical Linear Algebraã€‹
    
- Golubã€ŠMatrix Computationsã€‹
    

å†…å®¹ï¼š

- LUï¼ˆéƒ¨åˆ†é€‰ä¸»å…ƒï¼‰
    
- QRï¼ˆHouseholderï¼‰
    
- SVDï¼ˆæ¦‚å¿µçº§ï¼‰
    
- Condition number
    
- iterative æ–¹æ³•ï¼šCG / GMRES
    

é¡¹ç›®ï¼š

- ç”¨ QR å®ç°çº¿æ€§å›å½’
    
- ç”¨ CG è§£ç¨€ç–ç³»ç»Ÿ
    

---

# **ğŸ“˜ Week 16â€“20ï¼šBLAS/Eigen æ·±å…¥ï¼ˆ200hï¼‰**

å­¦ä¹  Eigen æºç è·¯å¾„ï¼ˆä½ æœ€å…³å¿ƒçš„éƒ¨åˆ†ï¼‰ï¼š

- DenseStorage
    
- MatrixBase / PlainObjectBase
    
- CwiseBinaryOp
    
- Map / Block / Transpose
    
- Packet moduleï¼ˆSIMDï¼‰
    

é¡¹ç›®ï¼š  
**å†™ä¸€ä¸ª mini-BLAS GEMMï¼ˆCPUï¼‰**

- pack
    
- block
    
- micro-kernelï¼ˆSIMDï¼‰
    
- macro-kernel
    
- parallel GEMM
    

äº§å‡ºï¼š

- è‡ªå·±å†™çš„ GEMM è¾¾åˆ° OpenBLAS 15%+
    

---

# ğŸŸ¦ **é˜¶æ®µ 4ï¼šCUDA + GPU å†…æ ¸ï¼ˆWeek 21â€“28ï¼Œå…± 8 å‘¨ï¼‰**

---

# **ğŸ“˜ Week 21â€“22ï¼šCUDA å…¥é—¨ï¼ˆ80hï¼‰**

å†…å®¹ï¼š

- grid/thread
    
- shared memory
    
- memory coalescing
    
- warp divergence
    
- warp shuffle
    

é¡¹ç›®ï¼š

- reductionï¼ˆwarp ä¼˜åŒ–ï¼‰
    
- transposeï¼ˆcoalescingï¼‰
    

---

# **ğŸ“˜ Week 23â€“24ï¼šCUDA é«˜çº§ï¼ˆ80hï¼‰**

å†…å®¹ï¼š

- async copy
    
- shared memory pipeline
    
- warpgroup MMAï¼ˆHopper/Blackwellï¼‰
    
- LDS / smem bank conflict
    
- parallel reduction tree
    

é¡¹ç›®ï¼š

- softmax kernelï¼ˆå¸¦ warp ä¼˜åŒ–ï¼‰
    
- layernorm kernel
    

---

# **ğŸ“˜ Week 25â€“28ï¼šCUTLASS æ·±å…¥ï¼ˆ160hï¼‰**

é˜…è¯» CUTLASS æºç ï¼š

- tile
    
- warp MMA
    
- epilogue
    
- iterator
    
- pipeline
    

é¡¹ç›®ï¼š  
**GPU GEMMï¼ˆmini-CUTLASSï¼‰**

- 64Ã—64Ã—32 tile
    
- shared memory double buffer
    
- warp-level MMA
    
- pipelined scheduler
    

æ€§èƒ½ç›®æ ‡ï¼šè¾¾åˆ° cuBLAS 30%â€“50%

---

# ğŸŸª **é˜¶æ®µ 5ï¼šAI Infra æ·±å…¥ï¼ˆWeek 29â€“40ï¼Œå…± 12 å‘¨ï¼‰**

---

# **ğŸ“˜ Week 29â€“32ï¼šPyTorch ATenï¼ˆ160hï¼‰**

é˜…è¯»ï¼š

- TensorImpl
    
- DispatchKey / Op Registration
    
- TensorIterator
    
- LazyTensor
    
- autograd engine æ‰©å±•
    

é¡¹ç›®ï¼š

- å†™ä¸€ä¸ªè‡ªå®šä¹‰ CUDA ç®—å­çš„ PyTorch æ‰©å±•
    
- å†™ tensor iterator kernel
    

---

# **ğŸ“˜ Week 33â€“36ï¼šç¼–è¯‘å™¨ï¼ˆMLIR/XLA/TVMï¼‰ï¼ˆ160hï¼‰**

å†…å®¹ï¼š

- IRï¼ˆSSAã€dialectï¼‰
    
- fusion / tiling pass
    
- buffer reuse
    
- schedule tree
    
- lowering pass
    

é¡¹ç›®ï¼š

- ç”¨ MLIR åšä¸€ä¸ª Matmul â†’ Tiled Matmul çš„ pass
    
- TVM auto-scheduler çš„è°ƒä¼˜å®éªŒ
    

---

# **ğŸ“˜ Week 37â€“40ï¼šTensorRT + vLLMï¼ˆ160hï¼‰**

å†…å®¹ï¼š

- TensorRT graph fusion
    
- quantization-aware optimization
    
- memory planner
    
- paged attentionï¼ˆvLLM æ ¸å¿ƒï¼‰
    

é¡¹ç›®ï¼š

- å†™ä¸€ä¸ª vLLM è‡ªå®šä¹‰ kernelï¼ˆåƒ PagedAttention ç®€åŒ–ç‰ˆï¼‰
    
- ä¼˜åŒ–æ¨ç† pipeline
    

---

# ğŸŸ¥ **é˜¶æ®µ 6ï¼šåˆ†å¸ƒå¼è®­ç»ƒï¼ˆWeek 41â€“48ï¼Œå…± 8 å‘¨ï¼‰**

---

# **ğŸ“˜ Week 41â€“44ï¼šPyTorch Distributed**

å†…å®¹ï¼š

- DDP
    
- ZeRO 1/2/3
    
- pipeline parallel
    
- tensor parallel
    
- activation checkpointing
    

é¡¹ç›®ï¼š

- å®ç°è‡ªå·±çš„ DDP-lite
    
- åœ¨ 2 å¡ä¸Šè®­ç»ƒ Llama éƒ¨åˆ†å±‚
    

---

# **ğŸ“˜ Week 45â€“48ï¼šè½»é‡çº§è®­ç»ƒå¼•æ“ï¼ˆLiteTrainï¼‰**

ä½ å°†å†™ä¸€ä¸ªç®€åŒ–ç‰ˆçš„è®­ç»ƒæ¡†æ¶ï¼š

åŠŸèƒ½åŒ…æ‹¬ï¼š

- æ•°æ®å¹¶è¡Œ
    
- æ¢¯åº¦è§„çº¦
    
- mixed precisionï¼ˆAMPï¼‰
    
- FP8/FP16 kernel
    

è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„ä½œå“é›†é¡¹ç›®ã€‚

---

# ğŸŸ¦ **é˜¶æ®µ 7ï¼šé¢è¯•å‡†å¤‡ï¼ˆWeek 49â€“52ï¼Œå…± 4 å‘¨ï¼‰**

---

# **ğŸ“˜ Week 49â€“50ï¼šHPC åŸºç¡€é¢è¯•**

- GEMM æ¨å¯¼
    
- cache blocking åŸç†
    
- SIMD ä¼˜åŒ–ç‚¹
    
- false sharing
    
- TLB miss / page walk
    

---

# **ğŸ“˜ Week 51ï¼šCUDA é¢è¯•**

- warp divergence
    
- SM occupancy
    
- memory coalescing
    
- softmax ç“¶é¢ˆä¼˜åŒ–
    
- é™„å¸¦ä¸Šæœºé¢˜ï¼šå†™ transpose + reduction + matmul å¾® kernel
    

---

# **ğŸ“˜ Week 52ï¼šSystem / PyTorch / Compiler é¢è¯•**

- DispatchKey path
    
- autograd backward graph
    
- mlir tiling
    
- kernel fusion
    
- vLLM å¦‚ä½•å‡å°‘ KV cache äº¤æ¢